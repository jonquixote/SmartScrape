{"data": {"entity_types": ["fiction_book", "bestseller_fiction_book", "product", "book"], "properties": ["title", "description", "url", "price", "brand", "rating", "image", "author", "asin", "number_of_reviews", "availability", "format"], "specific_criteria": {"price": {"max": 20}, "genre": "fiction", "bestseller": true, "format": ["paperback", "kindle", "hardcover"]}, "extraction_hints": {"website": "amazon.com", "pagination": "Amazon uses pagination. Look for the 'Next' button and handle pagination to scrape all pages of search results.", "sponsored_results": "Amazon displays sponsored results mixed with organic results.  Implement logic to identify and potentially filter or flag these as 'sponsored'. They often have a 'Sponsored' label or are visually different.", "price_variations": "Prices can vary based on format (Kindle, paperback, hardcover).  Extract the format along with the price and store it.", "asin": "ASIN (Amazon Standard Identification Number) is a unique identifier for each product on Amazon. It can usually be found in the URL or in the product details section. This is a great key for identifying unique books.", "rating_extraction": "Ratings are often displayed as a star rating and a number of reviews. Ensure both the rating value (e.g., 4.5 stars) and the number of reviews (e.g., 1234 ratings) are extracted.", "availability_extraction": "Check availability status (e.g., 'In Stock', 'Temporarily out of stock').  This information is crucial for understanding product availability.", "bestseller_badge": "Look for bestseller badges or indicators, they may be image-based or text-based.", "use_css_selectors": "Amazon's structure changes frequently. Rely on robust CSS selectors and consider using libraries that are resistant to HTML changes.", "dynamic_content": "Amazon heavily uses JavaScript to render content. Consider using a headless browser or a library that can execute JavaScript to ensure all relevant data is loaded before scraping.", "anti_scraping": "Amazon has anti-scraping measures.  Implement polite scraping techniques: respect robots.txt, introduce delays between requests, use rotating proxies, and set a reasonable user-agent."}}, "timestamp": 1746244851.490773, "context": null}